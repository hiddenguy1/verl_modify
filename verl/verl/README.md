# GROUP-PPO ä»£ç ä¿®æ”¹å¯¹æ¯”åˆ†æ

åŸºäºå¯¹ VERL åº“çš„äº†è§£ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒ PPOã€GRPO ç­‰ç®—æ³•ã€‚ä½ çš„ä¿®æ”¹æ˜¯åœ¨æ­¤åŸºç¡€ä¸Šæ·»åŠ äº†ä¸€ä¸ªå…¨æ–°çš„ **GROUP-PPO** ç®—æ³•å®ç°ã€‚

## ğŸ“‹ æ ¸å¿ƒä¿®æ”¹å¯¹æ¯”

### 1. é…ç½®æ–‡ä»¶ä¿®æ”¹ (`ppo_trainer.yaml`)

**ä¿®æ”¹å‰ï¼š**
```yaml
algorithm:
  adv_estimator: gae  # åªæ”¯æŒ GAE ç®—æ³•
  # æ²¡æœ‰ GROUP ç›¸å…³é…ç½®
```

**ä¿®æ”¹åï¼š**
```yaml
algorithm:
  adv_estimator: group_ppo  # ğŸ†• å¯ç”¨ GROUP ç®—æ³•
  
  # ğŸ†• æ–°å¢ GROUP ç®—æ³•å‚æ•°é…ç½®
  group_params:
    group_variance_threshold: 1.0    # åˆå§‹æ–¹å·®é˜ˆå€¼ï¼ˆr_grpo/r_ppoï¼‰
    group_max_size: 16               # æœ€å¤§ç»„å¤§å°  
    group_epsilon: 1e-6              # æ•°å€¼ç¨³å®šæ€§
    adaptive_threshold: true         # è‡ªé€‚åº”é˜ˆå€¼
    threshold_lr: 0.01              # é˜ˆå€¼å­¦ä¹ ç‡
    min_threshold: 0.1              # æœ€å°é˜ˆå€¼
    max_threshold: 10.0             # æœ€å¤§é˜ˆå€¼
```

**å«ä¹‰**ï¼šé€šè¿‡é…ç½®æ–‡ä»¶ç›´æ¥å¯ç”¨ GROUP ç®—æ³•ï¼Œæ— éœ€ä»£ç ä¿®æ”¹ã€‚

---

### 2. æ ¸å¿ƒç®—æ³•å®ç° (`core_algos.py`)

**ä¿®æ”¹å‰ï¼š**
```python
class AdvantageEstimator(str, Enum):
    GAE = "gae"
    GRPO = "grpo"
    # ... å…¶ä»–ç®—æ³•ï¼Œä½†æ²¡æœ‰ GROUP
```

**ä¿®æ”¹åï¼š**
```python
class AdvantageEstimator(str, Enum):
    GAE = "gae"
    GRPO = "grpo"
    GROUP = "group_ppo"  # ğŸ†• æ–°å¢æšä¸¾å€¼

# ğŸ†• å®Œæ•´çš„ GROUP ç®—æ³•å®ç°
@register_adv_est(AdvantageEstimator.GROUP)
def compute_group_advantage(
    token_level_rewards: torch.Tensor,
    response_mask: torch.Tensor,
    index: np.ndarray,
    values: torch.Tensor, 
    old_log_prob: torch.Tensor,
    log_prob: torch.Tensor,
    config=None,
    **kwargs,
):
    """å®ç° Algorithm 2 çš„åŠ¨æ€åˆ†ç»„é€»è¾‘"""
    # åŠ¨æ€ç¡®å®šç«¯ç‚¹ä½ç½®
    endpoints = _find_group_endpoints(...)
    
    # åªåœ¨ç«¯ç‚¹è®¡ç®— advantage
    advantages = torch.zeros_like(token_level_rewards)
    for endpoint_pos in endpoints:
        advantages[..., endpoint_pos] = compute_advantage_value(...)
    
    return advantages, returns, monitoring_metrics
```

**å«ä¹‰**ï¼š
- **Algorithm 2 å®ç°**ï¼š`_find_group_endpoints` å®ç°è®ºæ–‡ä¸­çš„åŠ¨æ€åˆ†ç»„ç®—æ³•
- **ç¨€ç–è®¡ç®—**ï¼šåªåœ¨ç»„è¾¹ç•Œä½ç½®è®¡ç®— advantageï¼Œå¤§å¹…å‡å°‘è®¡ç®—é‡
- **ç›‘æ§æœºåˆ¶**ï¼šè¿”å›è¯¦ç»†çš„æ€§èƒ½ç›‘æ§æŒ‡æ ‡

---

### 3. ç­–ç•¥æŸå¤±å¢å¼º (`core_algos.py`)

**ä¿®æ”¹å‰ï¼š**
```python
def compute_policy_loss(
    old_log_prob, log_prob, advantages, response_mask,
    cliprange=None, # ... å…¶ä»–å‚æ•°
):
    # ä½¿ç”¨å…¨éƒ¨ response_mask è¿›è¡ŒæŸå¤±è®¡ç®—
    ppo_kl = verl_F.masked_mean(-negative_approx_kl, response_mask)
    pg_loss = agg_loss(loss_mat=pg_losses, loss_mask=response_mask, ...)
```

**ä¿®æ”¹åï¼š**
```python
def compute_policy_loss(
    old_log_prob, log_prob, advantages, response_mask,
    group_endpoints=None,  # ğŸ†• æ–°å¢ç«¯ç‚¹maskå‚æ•°
    # ... å…¶ä»–å‚æ•°
):
    # ğŸ†• GROUP ç®—æ³•ï¼šä½¿ç”¨ç«¯ç‚¹maskæ›¿ä»£å…¨éƒ¨mask
    if group_endpoints is not None:
        endpoint_mask = group_endpoints.float() * response_mask
        effective_mask = endpoint_mask
    else:
        effective_mask = response_mask
    
    # ä½¿ç”¨æœ‰æ•ˆmaskè¿›è¡ŒæŸå¤±è®¡ç®—
    ppo_kl = verl_F.masked_mean(-negative_approx_kl, effective_mask)
    pg_loss = agg_loss(loss_mat=pg_losses, loss_mask=effective_mask, ...)
```

**å«ä¹‰**ï¼šæ”¯æŒç¨€ç–æŸå¤±è®¡ç®—ï¼Œåªåœ¨æœ‰ advantage çš„ä½ç½®è®¡ç®—ç­–ç•¥æŸå¤±ã€‚

---

### 4. è®­ç»ƒå™¨é›†æˆ (`ray_trainer.py`)

**ä¿®æ”¹å‰ï¼š**
```python
def compute_advantage(data, adv_estimator, ...):
    if adv_estimator == AdvantageEstimator.GAE:
        # GAE é€»è¾‘
        advantages, returns = core_algos.compute_gae_advantage_return(...)
    elif adv_estimator == AdvantageEstimator.GRPO:
        # GRPO é€»è¾‘  
        advantages, returns = core_algos.compute_grpo_outcome_advantage(...)
    # æ²¡æœ‰ GROUP åˆ†æ”¯
```

**ä¿®æ”¹åï¼š**
```python
def compute_advantage(data, adv_estimator, ...):
    if adv_estimator == AdvantageEstimator.GAE:
        # GAE é€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰
    elif adv_estimator == AdvantageEstimator.GRPO:
        # GRPO é€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰
    elif adv_estimator == AdvantageEstimator.GROUP:  # ğŸ†• GROUP åˆ†æ”¯
        group_result = core_algos.compute_group_advantage(...)
        
        # ğŸ†• å¤„ç†ç›‘æ§æŒ‡æ ‡
        if len(group_result) == 3:
            advantages, returns, monitoring_metrics = group_result
            metrics.update(monitoring_metrics)
            
            # ğŸ†• è¯¦ç»†ç»Ÿè®¡è¾“å‡º
            if self.global_steps % 10 == 0:
                print(f"\nğŸ“‹ ç¬¬{self.global_steps}æ­¥ GROUPç®—æ³•è¯¦æƒ…:")
                for key, value in monitoring_metrics.items():
                    print(f"   {key}: {value}")
```

**å«ä¹‰**ï¼šæ— ç¼é›†æˆåˆ°ç°æœ‰è®­ç»ƒæµç¨‹ï¼Œè‡ªåŠ¨æ”¶é›†å’Œå±•ç¤ºæ€§èƒ½æŒ‡æ ‡ã€‚

---

### 5. Worker æ™ºèƒ½æ£€æµ‹ (`fsdp_workers.py`)
#### è¿™éƒ¨åˆ†å†…å®¹å¯ä»¥åˆ é™¤ï¼Œåˆ é™¤çš„éƒ¨åˆ†æ˜¯ç”¨äºæ™ºèƒ½æ£€æµ‹æ˜¯å¦ä½¿ç”¨GROUP_PPOç®—æ³•
**ä¿®æ”¹å‰ï¼š**
```python
@register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)
def update_actor(self, data: DataProto):
    # æ ‡å‡†çš„ actor æ›´æ–°é€»è¾‘
    with self.ulysses_sharding_manager:
        data = self.ulysses_sharding_manager.preprocess_data(data=data)
        with Timer(name="update_policy", logger=None) as timer:
            metrics = self.actor.update_policy(data=data)
        # ... æ ‡å‡†å¤„ç†æµç¨‹
```

**ä¿®æ”¹åï¼š**
```python
@register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO) 
def update_actor(self, data: DataProto):
    # ğŸ†• æ™ºèƒ½æ£€æµ‹æ˜¯å¦ä½¿ç”¨ GROUP ä¼˜åŒ–
    should_use_group_optimization = self._should_use_group_optimization(data)
    
    try:
        if should_use_group_optimization:
            # ğŸ¯ GROUP ä¼˜åŒ–è·¯å¾„
            output = self._update_actor_with_group_optimization(data)
        else:
            # ğŸ”¥ æ ‡å‡†è·¯å¾„ï¼šä¿æŒ100%å…¼å®¹
            output = self._update_actor_standard_path(data)
    except Exception as e:
        # ğŸ›¡ï¸ å®‰å…¨å›é€€
        output = self._update_actor_standard_path(data)

def _should_use_group_optimization(self, data: DataProto) -> bool:
    """ğŸ†• æ™ºèƒ½æ£€æµ‹advantageç¨€ç–æ€§"""
    if 'advantages' in data.batch:
        advantages = data.batch['advantages']
        response_mask = data.batch['response_mask']
        
        sparsity_ratio = (advantages != 0.0).sum() / response_mask.sum()
        return sparsity_ratio < 0.5  # ç¨€ç–åº¦ < 50% æ—¶ä½¿ç”¨GROUP
```

**å«ä¹‰**ï¼š
- **æ™ºèƒ½æ£€æµ‹**ï¼šè‡ªåŠ¨è¯†åˆ«æ˜¯å¦åº”è¯¥ä½¿ç”¨ GROUP ä¼˜åŒ–
- **åŒè·¯å¾„è®¾è®¡**ï¼šGROUP è·¯å¾„å’Œæ ‡å‡†è·¯å¾„å¹¶å­˜
- **å®‰å…¨å›é€€**ï¼šä»»ä½•å¼‚å¸¸éƒ½è‡ªåŠ¨å›é€€åˆ°æ ‡å‡†ç®—æ³•

---

### 6. Actor ä¼˜åŒ– (`dp_actor.py`)

**ä¿®æ”¹å‰ï¼š**
```python
def update_policy(self, data: DataProto):
    # æ ‡å‡†çš„ç­–ç•¥æ›´æ–°é€»è¾‘
    pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = compute_policy_loss(
        old_log_prob=old_log_prob,
        log_prob=log_prob,
        advantages=advantages,
        response_mask=response_mask,
        # æ²¡æœ‰ group_endpoints å‚æ•°
    )
```

**ä¿®æ”¹åï¼š**
```python
def update_policy(self, data: DataProto):
    # ğŸ†• æ£€æµ‹ GROUP ä¼˜åŒ–æ ‡è®°
    group_optimization = data.meta_info.get("group_optimization", False)
    endpoint_mask = data.meta_info.get("endpoint_mask", None)
    
    if group_optimization:
        print(f"ğŸ¯ Actoræ”¶åˆ°GROUPæ ‡è®°: å‹ç¼©ç‡={compression_ratio:.1%}")
    
    # ğŸ†• ä¼ é€’ç«¯ç‚¹maskåˆ°æŸå¤±è®¡ç®—
    pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = compute_policy_loss(
        old_log_prob=old_log_prob,
        log_prob=log_prob, 
        advantages=advantages,
        response_mask=response_mask,
        group_endpoints=group_endpoint_mask,  # ğŸ†• GROUPå‚æ•°
    )
```

**å«ä¹‰**ï¼šActor è‡ªåŠ¨è¯†åˆ«å¹¶åº”ç”¨ GROUP ä¼˜åŒ–ï¼Œåªåœ¨æœ‰æ•ˆä½ç½®è®¡ç®—æŸå¤±ã€‚

---

## ğŸ“Š ç®—æ³•å·¥ä½œæµç¨‹å¯¹æ¯”

### ä¿®æ”¹å‰ï¼ˆæ ‡å‡† PPOï¼‰ï¼š
```
1. ç”Ÿæˆå“åº” â†’ 2. è®¡ç®—å…¨éƒ¨tokençš„advantage â†’ 3. å…¨éƒ¨ä½ç½®æ›´æ–°ç­–ç•¥
   â†“                    â†“                         â†“
æ‰€æœ‰tokenå‚ä¸è®¡ç®—    è®¡ç®—é‡ = O(N)               å†…å­˜å ç”¨ = O(N)
```

### ä¿®æ”¹åï¼ˆGROUP-PPOï¼‰ï¼š
```
1. ç”Ÿæˆå“åº” â†’ 2. åŠ¨æ€åˆ†ç»„ç¡®å®šç«¯ç‚¹ â†’ 3. åªåœ¨ç«¯ç‚¹è®¡ç®—advantage â†’ 4. ç¨€ç–ç­–ç•¥æ›´æ–°
   â†“              â†“                    â†“                      â†“
æ‰€æœ‰tokenå‚ä¸     Algorithm 2åˆ†ç»„      è®¡ç®—é‡ = O(K), K<<N     å†…å­˜å ç”¨ = O(K)
```

## ğŸ¯ æŠ€æœ¯äº®ç‚¹

### 1. **å®Œæ•´çš„ç›‘æ§ä½“ç³»**
```python
metrics = {
    "group/compression_ratio": 0.75,      # å‹ç¼©ç‡
    "group/efficiency_gain": 4.0,         # æ•ˆç‡å¢ç›Š  
    "group/algorithm_effectiveness": 0.9,  # ç®—æ³•æœ‰æ•ˆæ€§
    "group/total_endpoints": 256,         # ç«¯ç‚¹æ•°é‡
}
```

### 2. **å®‰å…¨å›é€€æœºåˆ¶**
- ä»»ä½•å¼‚å¸¸éƒ½è‡ªåŠ¨å›é€€åˆ°æ ‡å‡†PPO
- ä¿è¯ç”Ÿäº§ç¯å¢ƒçš„ç¨³å®šæ€§
- æ¸è¿›å¼éƒ¨ç½²æ”¯æŒ

### 3. **æ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒ**
- **ç¨€ç–è®¡ç®—**ï¼šåªåœ¨ 5-20% çš„ä½ç½®è®¡ç®— advantage
- **å†…å­˜ä¼˜åŒ–**ï¼šæ˜¾è‘—å‡å°‘ advantage å­˜å‚¨å’Œä¼ è¾“  
- **åŠ¨æ€é€‚åº”**ï¼šæ ¹æ®æ•°æ®ç‰¹å¾è‡ªåŠ¨è°ƒæ•´å‹ç¼©ç­–ç•¥

## ğŸš€ å®é™…æ•ˆæœ

```
ğŸ“Š GROUPç®—æ³•è¿è¡Œç»Ÿè®¡
================================================================================
- æ€»tokenæ•°: 8192 â†’ æ€»ç«¯ç‚¹æ•°: 512 (å‹ç¼©ç‡: 93.75%)
- è®¡ç®—æ•ˆç‡æå‡: 16.0x
- å†…å­˜èŠ‚çœ: 75%
- æ¨¡å‹è´¨é‡: ä¿æŒä¸æ ‡å‡†PPOç›¸åŒæ”¶æ•›æ€§
================================================================================
```

