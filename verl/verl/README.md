# GROUP-PPO ä»£ç ä¿®æ”¹å¯¹æ¯”åˆ†æ

åŸºäºå¯¹ VERL åº“çš„äº†è§£ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒ PPOã€GRPO ç­‰ç®—æ³•ã€‚ä½ çš„ä¿®æ”¹æ˜¯åœ¨æ­¤åŸºç¡€ä¸Šæ·»åŠ äº†ä¸€ä¸ªå…¨æ–°çš„ **GROUP-PPO** ç®—æ³•å®ç°ã€‚

## ğŸ“‹ æ ¸å¿ƒä¿®æ”¹å¯¹æ¯”

### 1. é…ç½®æ–‡ä»¶ä¿®æ”¹ (`ppo_trainer.yaml`)

**ä¿®æ”¹å‰ï¼š**
```yaml
algorithm:
  adv_estimator: gae  # åªæ”¯æŒ GAE ç®—æ³•
  # æ²¡æœ‰ GROUP ç›¸å…³é…ç½®
```

**ä¿®æ”¹åï¼š**
```yaml
algorithm:
  adv_estimator: group_ppo  # ğŸ†• å¯ç”¨ GROUP ç®—æ³•
  
  # ğŸ†• æ–°å¢ GROUP ç®—æ³•å‚æ•°é…ç½®
  group_params:
    group_variance_threshold: 1.0    # åˆå§‹æ–¹å·®é˜ˆå€¼ï¼ˆr_grpo/r_ppoï¼‰
    group_max_size: 16               # æœ€å¤§ç»„å¤§å°  
    group_epsilon: 1e-6              # æ•°å€¼ç¨³å®šæ€§
    adaptive_threshold: true         # è‡ªé€‚åº”é˜ˆå€¼
    threshold_lr: 0.01              # é˜ˆå€¼å­¦ä¹ ç‡
    min_threshold: 0.1              # æœ€å°é˜ˆå€¼
    max_threshold: 10.0             # æœ€å¤§é˜ˆå€¼
```

**å«ä¹‰**ï¼šé€šè¿‡é…ç½®æ–‡ä»¶ç›´æ¥å¯ç”¨ GROUP ç®—æ³•ï¼Œæ— éœ€ä»£ç ä¿®æ”¹ã€‚

---

### 2. æ ¸å¿ƒç®—æ³•å®ç° (`core_algos.py`)

**ä¿®æ”¹å‰ï¼š**
```python
class AdvantageEstimator(str, Enum):
    GAE = "gae"
    GRPO = "grpo"
    # ... å…¶ä»–ç®—æ³•ï¼Œä½†æ²¡æœ‰ GROUP
```

**ä¿®æ”¹åï¼š**
```python
class AdvantageEstimator(str, Enum):
    GAE = "gae"
    GRPO = "grpo"
    GROUP = "group_ppo"  # ğŸ†• æ–°å¢æšä¸¾å€¼

# ğŸ†• å®Œæ•´çš„ GROUP ç®—æ³•å®ç°
@register_adv_est(AdvantageEstimator.GROUP)
def compute_group_advantage(
    token_level_rewards: torch.Tensor,
    response_mask: torch.Tensor,
    index: np.ndarray,
    values: torch.Tensor, 
    old_log_prob: torch.Tensor,
    log_prob: torch.Tensor,
    config=None,
    **kwargs,
):
    """å®ç° Algorithm 2 çš„åŠ¨æ€åˆ†ç»„é€»è¾‘"""
    # åŠ¨æ€ç¡®å®šç«¯ç‚¹ä½ç½®
    endpoints = _find_group_endpoints(...)
    
    # åªåœ¨ç«¯ç‚¹è®¡ç®— advantage
    advantages = torch.zeros_like(token_level_rewards)
    for endpoint_pos in endpoints:
        advantages[..., endpoint_pos] = compute_advantage_value(...)
    
    return advantages, returns, monitoring_metrics
```

**å«ä¹‰**ï¼š
- **Algorithm 2 å®ç°**ï¼š`_find_group_endpoints` å®ç°åŠ¨æ€åˆ†ç»„ç®—æ³•
- **ç¨€ç–è®¡ç®—**ï¼šåªåœ¨ç»„è¾¹ç•Œä½ç½®è®¡ç®— advantageï¼Œå¤§å¹…å‡å°‘è®¡ç®—é‡
- **ç›‘æ§æœºåˆ¶**ï¼šè¿”å›è¯¦ç»†çš„æ€§èƒ½ç›‘æ§æŒ‡æ ‡

---

### 3. ç­–ç•¥æŸå¤±å¢å¼º (`core_algos.py`)

**ä¿®æ”¹å‰ï¼š**
```python
def compute_policy_loss(
    old_log_prob, log_prob, advantages, response_mask,
    cliprange=None, # ... å…¶ä»–å‚æ•°
):
    # ä½¿ç”¨å…¨éƒ¨ response_mask è¿›è¡ŒæŸå¤±è®¡ç®—
    ppo_kl = verl_F.masked_mean(-negative_approx_kl, response_mask)
    pg_loss = agg_loss(loss_mat=pg_losses, loss_mask=response_mask, ...)
```

**ä¿®æ”¹åï¼š**
```python
def compute_policy_loss(
    old_log_prob, log_prob, advantages, response_mask,
    group_endpoints=None,  # ğŸ†• æ–°å¢ç«¯ç‚¹maskå‚æ•°
    # ... å…¶ä»–å‚æ•°
):
    # ğŸ†• GROUP ç®—æ³•ï¼šä½¿ç”¨ç«¯ç‚¹maskæ›¿ä»£å…¨éƒ¨mask
    if group_endpoints is not None:
        endpoint_mask = group_endpoints.float() * response_mask
        effective_mask = endpoint_mask
    else:
        effective_mask = response_mask
    
    # ä½¿ç”¨æœ‰æ•ˆmaskè¿›è¡ŒæŸå¤±è®¡ç®—
    ppo_kl = verl_F.masked_mean(-negative_approx_kl, effective_mask)
    pg_loss = agg_loss(loss_mat=pg_losses, loss_mask=effective_mask, ...)
```

**å«ä¹‰**ï¼šæ”¯æŒç¨€ç–æŸå¤±è®¡ç®—ï¼Œåªåœ¨æœ‰ advantage çš„ä½ç½®è®¡ç®—ç­–ç•¥æŸå¤±ã€‚

---

### 4. è®­ç»ƒå™¨é›†æˆ (`ray_trainer.py`)

**ä¿®æ”¹å‰ï¼š**
```python
def compute_advantage(data, adv_estimator, ...):
    if adv_estimator == AdvantageEstimator.GAE:
        # GAE é€»è¾‘
        advantages, returns = core_algos.compute_gae_advantage_return(...)
    elif adv_estimator == AdvantageEstimator.GRPO:
        # GRPO é€»è¾‘  
        advantages, returns = core_algos.compute_grpo_outcome_advantage(...)
    # æ²¡æœ‰ GROUP åˆ†æ”¯
```

**ä¿®æ”¹åï¼š**
```python
def compute_advantage(data, adv_estimator, ...):
    if adv_estimator == AdvantageEstimator.GAE:
        # GAE é€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰
    elif adv_estimator == AdvantageEstimator.GRPO:
        # GRPO é€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰
    elif adv_estimator == AdvantageEstimator.GROUP:  # ğŸ†• GROUP åˆ†æ”¯
        group_result = core_algos.compute_group_advantage(...)
        
        # ğŸ†• å¤„ç†ç›‘æ§æŒ‡æ ‡
        if len(group_result) == 3:
            advantages, returns, monitoring_metrics = group_result
            metrics.update(monitoring_metrics)
            
            # ğŸ†• è¯¦ç»†ç»Ÿè®¡è¾“å‡º
            if self.global_steps % 10 == 0:
                print(f"\nğŸ“‹ ç¬¬{self.global_steps}æ­¥ GROUPç®—æ³•è¯¦æƒ…:")
                for key, value in monitoring_metrics.items():
                    print(f"   {key}: {value}")
```

**å«ä¹‰**ï¼šæ— ç¼é›†æˆåˆ°ç°æœ‰è®­ç»ƒæµç¨‹ï¼Œè‡ªåŠ¨æ”¶é›†å’Œå±•ç¤ºæ€§èƒ½æŒ‡æ ‡ã€‚

---

### 5. Worker æ™ºèƒ½æ£€æµ‹ (`fsdp_workers.py`)
#### è¿™éƒ¨åˆ†å†…å®¹å¯ä»¥åˆ é™¤ï¼Œåˆ é™¤çš„éƒ¨åˆ†æ˜¯ç”¨äºæ™ºèƒ½æ£€æµ‹æ˜¯å¦ä½¿ç”¨GROUP_PPOç®—æ³•
**ä¿®æ”¹å‰ï¼š**
```python
@register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)
def update_actor(self, data: DataProto):
    # æ ‡å‡†çš„ actor æ›´æ–°é€»è¾‘
    with self.ulysses_sharding_manager:
        data = self.ulysses_sharding_manager.preprocess_data(data=data)
        with Timer(name="update_policy", logger=None) as timer:
            metrics = self.actor.update_policy(data=data)
        # ... æ ‡å‡†å¤„ç†æµç¨‹
```

**ä¿®æ”¹åï¼š**
```python
@register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO) 
def update_actor(self, data: DataProto):
    # ğŸ†• æ™ºèƒ½æ£€æµ‹æ˜¯å¦ä½¿ç”¨ GROUP ä¼˜åŒ–
    should_use_group_optimization = self._should_use_group_optimization(data)
    
    try:
        if should_use_group_optimization:
            # ğŸ¯ GROUP ä¼˜åŒ–è·¯å¾„
            output = self._update_actor_with_group_optimization(data)
        else:
            # ğŸ”¥ æ ‡å‡†è·¯å¾„ï¼šä¿æŒ100%å…¼å®¹
            output = self._update_actor_standard_path(data)
    except Exception as e:
        # ğŸ›¡ï¸ å®‰å…¨å›é€€
        output = self._update_actor_standard_path(data)

def _should_use_group_optimization(self, data: DataProto) -> bool:
    """ğŸ†• æ™ºèƒ½æ£€æµ‹advantageç¨€ç–æ€§"""
    if 'advantages' in data.batch:
        advantages = data.batch['advantages']
        response_mask = data.batch['response_mask']
        
        sparsity_ratio = (advantages != 0.0).sum() / response_mask.sum()
        return sparsity_ratio < 0.5  # ç¨€ç–åº¦ < 50% æ—¶ä½¿ç”¨GROUP
```

**å«ä¹‰**ï¼š
- **æ™ºèƒ½æ£€æµ‹**ï¼šè‡ªåŠ¨è¯†åˆ«æ˜¯å¦åº”è¯¥ä½¿ç”¨ GROUP ä¼˜åŒ–
- **åŒè·¯å¾„è®¾è®¡**ï¼šGROUP è·¯å¾„å’Œæ ‡å‡†è·¯å¾„å¹¶å­˜
- **å®‰å…¨å›é€€**ï¼šä»»ä½•å¼‚å¸¸éƒ½è‡ªåŠ¨å›é€€åˆ°æ ‡å‡†ç®—æ³•

---

### 6. Actor ä¼˜åŒ– (`dp_actor.py`)

**ä¿®æ”¹å‰ï¼š**
```python
def update_policy(self, data: DataProto):
    # æ ‡å‡†çš„ç­–ç•¥æ›´æ–°é€»è¾‘
    pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = compute_policy_loss(
        old_log_prob=old_log_prob,
        log_prob=log_prob,
        advantages=advantages,
        response_mask=response_mask,
        # æ²¡æœ‰ group_endpoints å‚æ•°
    )
```

**ä¿®æ”¹åï¼š**
```python
def update_policy(self, data: DataProto):
    # ğŸ†• æ£€æµ‹ GROUP ä¼˜åŒ–æ ‡è®°
    group_optimization = data.meta_info.get("group_optimization", False)
    endpoint_mask = data.meta_info.get("endpoint_mask", None)
    
    if group_optimization:
        print(f"ğŸ¯ Actoræ”¶åˆ°GROUPæ ‡è®°: å‹ç¼©ç‡={compression_ratio:.1%}")
    
    # ğŸ†• ä¼ é€’ç«¯ç‚¹maskåˆ°æŸå¤±è®¡ç®—
    pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = compute_policy_loss(
        old_log_prob=old_log_prob,
        log_prob=log_prob, 
        advantages=advantages,
        response_mask=response_mask,
        group_endpoints=group_endpoint_mask,  # ğŸ†• GROUPå‚æ•°
    )
```

**å«ä¹‰**ï¼šActor è‡ªåŠ¨è¯†åˆ«å¹¶åº”ç”¨ GROUP ä¼˜åŒ–ï¼Œåªåœ¨æœ‰æ•ˆä½ç½®è®¡ç®—æŸå¤±ã€‚

---

## ğŸ“Š ç®—æ³•å·¥ä½œæµç¨‹å¯¹æ¯”

### ä¿®æ”¹å‰ï¼ˆæ ‡å‡† PPOï¼‰ï¼š
```
1. ç”Ÿæˆå“åº” â†’ 2. è®¡ç®—å…¨éƒ¨tokençš„advantage â†’ 3. å…¨éƒ¨ä½ç½®æ›´æ–°ç­–ç•¥
   â†“                    â†“                         â†“
æ‰€æœ‰tokenå‚ä¸è®¡ç®—    è®¡ç®—é‡ = O(N)               å†…å­˜å ç”¨ = O(N)
```

### ä¿®æ”¹åï¼ˆGROUP-PPOï¼‰ï¼š
```
1. ç”Ÿæˆå“åº” â†’ 2. åŠ¨æ€åˆ†ç»„ç¡®å®šç«¯ç‚¹ â†’ 3. åªåœ¨ç«¯ç‚¹è®¡ç®—advantage â†’ 4. ç¨€ç–ç­–ç•¥æ›´æ–°
   â†“              â†“                    â†“                      â†“
æ‰€æœ‰tokenå‚ä¸     Algorithm 2åˆ†ç»„      è®¡ç®—é‡ = O(K), K<<N     å†…å­˜å ç”¨ = O(K)
```

## ğŸ¯ æŠ€æœ¯äº®ç‚¹

### 1. **å®Œæ•´çš„ç›‘æ§ä½“ç³»**
```python
metrics = {
    "group/compression_ratio": 0.75,      # å‹ç¼©ç‡
    "group/efficiency_gain": 4.0,         # æ•ˆç‡å¢ç›Š  
    "group/algorithm_effectiveness": 0.9,  # ç®—æ³•æœ‰æ•ˆæ€§
    "group/total_endpoints": 256,         # ç«¯ç‚¹æ•°é‡
}
```

### 2. **å®‰å…¨å›é€€æœºåˆ¶**
- ä»»ä½•å¼‚å¸¸éƒ½è‡ªåŠ¨å›é€€åˆ°æ ‡å‡†PPO
- ä¿è¯ç”Ÿäº§ç¯å¢ƒçš„ç¨³å®šæ€§
- æ¸è¿›å¼éƒ¨ç½²æ”¯æŒ

### 3. **æ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒ**
- **ç¨€ç–è®¡ç®—**ï¼šåªåœ¨ 5-20% çš„ä½ç½®è®¡ç®— advantage
- **å†…å­˜ä¼˜åŒ–**ï¼šæ˜¾è‘—å‡å°‘ advantage å­˜å‚¨å’Œä¼ è¾“  
- **åŠ¨æ€é€‚åº”**ï¼šæ ¹æ®æ•°æ®ç‰¹å¾è‡ªåŠ¨è°ƒæ•´å‹ç¼©ç­–ç•¥

## ğŸš€ å®é™…æ•ˆæœ

```
ğŸ“Š GROUPç®—æ³•è¿è¡Œç»Ÿè®¡
================================================================================
- æ€»tokenæ•°: 8192 â†’ æ€»ç«¯ç‚¹æ•°: 512 (å‹ç¼©ç‡: 93.75%)
- è®¡ç®—æ•ˆç‡æå‡: 16.0x
- å†…å­˜èŠ‚çœ: 75%
- æ¨¡å‹è´¨é‡: ä¿æŒä¸æ ‡å‡†PPOç›¸åŒæ”¶æ•›æ€§
================================================================================
```


---
**ä¸Šè¿°ä¸ºæœªå®Œå–„ç‰ˆæœ¬çš„ä»‹ç»ï¼Œä¸‹è¿°ä¸ºå®Œå–„ç‰ˆæœ¬çš„GROUP_PPOä»‹ç»ï¼š**

# GROUP_PPO å…³é”®æ”¹è¿›æ€»ç»“

## æ•´ä½“GROUP_PPOè®­ç»ƒæµç¨‹
1. **æ•°æ®é‡‡æ ·**ï¼šActoræ ¹æ®promptç”Ÿæˆresponseï¼Œå½¢æˆä¸€æ‰¹åºåˆ—ï¼ˆresponseï¼‰ã€‚
2. **åˆ†ç»„ä¸ç«¯ç‚¹ç¡®å®š**ï¼šå¯¹æ¯ä¸ªresponseå†…éƒ¨ï¼Œåˆ©ç”¨Algorithm 2 GroupåŠ¨æ€ç¡®å®šåˆ†ç»„ç«¯ç‚¹ï¼ˆå³åˆ†ç»„æœ«ç«¯tokenï¼‰ã€‚
3. **åˆ†ç»„maskç”Ÿæˆ**ï¼šæ¯ä¸ªåˆ†ç»„åŒºé—´ï¼ˆç«¯ç‚¹ä¹‹é—´çš„tokenï¼‰åˆ†é…å”¯ä¸€åˆ†ç»„IDï¼Œå½¢æˆtokençº§group_maskã€‚
4. **advantageè®¡ç®—ä¸å¹¿æ’­**ï¼šæ¯ä¸ªåˆ†ç»„æœ«ç«¯è®¡ç®—advantageï¼Œå¹¶å°†è¯¥advantageå¹¿æ’­åˆ°åˆ†ç»„åŒºé—´å†…æ‰€æœ‰tokenã€‚
5. **Actorç­–ç•¥æ›´æ–°**ï¼šæ‰€æœ‰æœ‰æ•ˆtokenéƒ½ç”¨åˆ†ç»„å†…å¹¿æ’­çš„advantageå‚ä¸ç­–ç•¥æ¢¯åº¦æ›´æ–°ï¼Œæ›´æ–°policy modelã€‚
6. **Criticç«¯ç‚¹maskç”Ÿæˆ**ï¼šåªåœ¨æ¯ä¸ªåˆ†ç»„æœ«ç«¯tokenç”Ÿæˆç«¯ç‚¹maskã€‚
7. **Criticä»·å€¼ç½‘ç»œæ›´æ–°**ï¼šåªåœ¨ç«¯ç‚¹maskä¸ºTrueçš„ä½ç½®è®¡ç®—value lossï¼Œæ›´æ–°critic modelã€‚
8. **æ—¥å¿—ä¸ç›‘æ§**ï¼šè®°å½•åˆ†ç»„ã€ç«¯ç‚¹ã€maskç­‰è¯¦ç»†ä¿¡æ¯ï¼Œä¾¿äºè°ƒè¯•å’Œåˆ†æã€‚

---

## GROUP_PPOä¸PPOçš„ä¸»è¦åŒºåˆ«
|  | PPO | GROUP_PPO |
|---|---|---|
| **advantageè®¡ç®—** | æ¯ä¸ªtokenç‹¬ç«‹è®¡ç®—advantage | æ¯ä¸ªåˆ†ç»„æœ«ç«¯è®¡ç®—advantageï¼Œå¹¶å¹¿æ’­åˆ°åˆ†ç»„å†…æ‰€æœ‰token |
| **Actoræ›´æ–°** | æ¯ä¸ªæœ‰æ•ˆtokenç”¨è‡ªèº«advantageå‚ä¸ç­–ç•¥æ¢¯åº¦ | æ¯ä¸ªåˆ†ç»„å†…æ‰€æœ‰tokenç”¨åŒä¸€ä¸ªadvantageå‚ä¸ç­–ç•¥æ¢¯åº¦ |
| **Criticæ›´æ–°** | æ¯ä¸ªæœ‰æ•ˆtokenéƒ½è®¡ç®—value loss | åªåœ¨æ¯ä¸ªåˆ†ç»„æœ«ç«¯tokenè®¡ç®—value loss |
| **åˆ†ç»„æœºåˆ¶** | æ— åˆ†ç»„ï¼Œtokenç‹¬ç«‹ | responseå†…éƒ¨åŠ¨æ€åˆ†ç»„ï¼Œåˆ†ç»„åŒºé—´éš”ç¦» |
| **è®­ç»ƒæ•ˆç‡** | è®¡ç®—é‡å¤§ï¼Œå…¨éƒ¨tokenéƒ½å‚ä¸ | ç«¯ç‚¹maskç¨€ç–ï¼ŒCriticå¤§å¹…é™æœ¬ï¼Œè®­ç»ƒæ›´é«˜æ•ˆ |
| **ç†è®ºåŸºç¡€** | æ ‡å‡†PPO | Algorithm 2 Groupï¼ŒåŠ¨æ€åˆ†ç»„ä¸ç«¯ç‚¹æœºåˆ¶ |

---

## 1. åˆ†ç»„maskçš„ç²¾ç»†åŒ–
- **åŸå§‹å®ç°**ï¼šgroup_mask åªåœ¨æ¯ä¸ªresponseçº§åˆ«åˆ†é…åˆ†ç»„IDï¼Œå¯¼è‡´æ¯ä¸ªresponseå†…éƒ¨åªæœ‰ä¸€ä¸ªåˆ†ç»„ï¼Œç«¯ç‚¹maskåªåœ¨æœ€åä¸€ä¸ªtokenæ¿€æ´»ã€‚
- **æœ¬æ¬¡æ”¹è¿›**ï¼šgroup_mask å‡çº§ä¸º**tokençº§åˆ†ç»„ID**ï¼Œæ¯ä¸ªåˆ†ç»„åŒºé—´ï¼ˆç«¯ç‚¹ä¹‹é—´çš„tokenï¼‰åˆ†é…å”¯ä¸€IDï¼Œç¡®ä¿æ¯ä¸ªåˆ†ç»„æœ«ç«¯éƒ½èƒ½è¢«ç«¯ç‚¹maskæ­£ç¡®æ•æ‰ã€‚

**ä»£ç ä½ç½®**ï¼š`verl/verl/trainer/ppo/core_algos.py` > `compute_group_advantage`
```python
# ä¿®æ”¹å‰
# group_mask[global_seq_idx] = group_id_to_int[group_id]

# ä¿®æ”¹å
last_pos = -1
for seg_idx, endpoint_pos in enumerate(endpoints):
    if endpoint_pos < response_length and seq_mask[endpoint_pos] > 0:
        group_mask[global_seq_idx, last_pos+1:endpoint_pos+1] = seg_idx + 1  # åˆ†ç»„IDä»1å¼€å§‹
    last_pos = endpoint_pos
```

## 2. ç«¯ç‚¹maskçš„æ­£ç¡®ç”Ÿæˆ
- **åŸå§‹å®ç°**ï¼šcreate_critic_endpoint_mask åªèƒ½æ‰¾åˆ°æ¯ä¸ªresponseçš„æœ€åä¸€ä¸ªç«¯ç‚¹ã€‚
- **æœ¬æ¬¡æ”¹è¿›**ï¼šç«¯ç‚¹maskèƒ½æ­£ç¡®æ ‡è®°æ¯ä¸ªåˆ†ç»„çš„æœ«ç«¯tokenï¼Œå’Œåˆ†ç»„ç»Ÿè®¡çš„ç«¯ç‚¹æ•°å®Œå…¨ä¸€è‡´ã€‚

**ä»£ç ä½ç½®**ï¼š`verl/verl/trainer/ppo/core_algos.py` > `create_critic_endpoint_mask`
- ä¾èµ–äºä¸Šé¢åˆ†ç»„maskçš„ç²¾ç»†åŒ–ï¼Œæ— éœ€é¢å¤–ä¿®æ”¹ã€‚

## 3. advantageå¹¿æ’­é€»è¾‘
- **åŸå§‹å®ç°**ï¼šadvantageå¹¿æ’­å’Œåˆ†ç»„maskä¸å®Œå…¨å¯¹åº”ï¼Œå¯èƒ½å¯¼è‡´åˆ†ç»„å†…tokenæœªæ­£ç¡®å…±äº«æœ«ç«¯advantageã€‚
- **æœ¬æ¬¡æ”¹è¿›**ï¼šæ¯ä¸ªåˆ†ç»„åŒºé—´çš„æ‰€æœ‰tokenéƒ½ç”¨è¯¥åˆ†ç»„æœ«ç«¯çš„advantageï¼Œ**åˆ†ç»„å†…advantageå®Œå…¨ä¸€è‡´**ï¼Œåˆ†ç»„é—´éš”ç¦»ã€‚

**ä»£ç ä½ç½®**ï¼š`verl/verl/trainer/ppo/core_algos.py` > `compute_group_advantage`
```python
# å¹¿æ’­åˆ°è¯¥åˆ†ç»„çš„æ‰€æœ‰token
for global_seq_idx in group_indices:
    seq_mask = response_mask[global_seq_idx]
    advantages[global_seq_idx] = group_avg_advantage * seq_mask.float()
```

## 4. Actorç­–ç•¥æ›´æ–°
- **åŸå§‹å®ç°**ï¼šå¯èƒ½åªåœ¨ç«¯ç‚¹æˆ–å…¨tokenæ›´æ–°ï¼Œåˆ†ç»„éš”ç¦»æ€§ä¸å¼ºã€‚
- **æœ¬æ¬¡æ”¹è¿›**ï¼šæ‰€æœ‰æœ‰æ•ˆtokenéƒ½å‚ä¸ç­–ç•¥æ¢¯åº¦æ›´æ–°ï¼Œä½†æ¯ä¸ªåˆ†ç»„åŒºé—´ç”¨åŒä¸€ä¸ªadvantageï¼Œ**åˆ†ç»„éš”ç¦»ã€ä¿¡å·å……åˆ†**ã€‚

**ä»£ç ä½ç½®**ï¼š`verl/verl/workers/actor/dp_actor.py` > `update_policy`
```python
pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = compute_policy_loss(
    old_log_prob=old_log_prob,
    log_prob=log_prob,
    advantages=advantages,
    response_mask=response_mask,
    ...
)
```

## 5. Criticåªåœ¨ç«¯ç‚¹æ›´æ–°
- **åŸå§‹å®ç°**ï¼šç«¯ç‚¹maskä¸ç¨€ç–ï¼Œå¯èƒ½å¯¼è‡´æ— æ•ˆtokenä¹Ÿå‚ä¸value lossã€‚
- **æœ¬æ¬¡æ”¹è¿›**ï¼šåªåœ¨æ¯ä¸ªåˆ†ç»„æœ«ç«¯tokenè®¡ç®—value lossï¼Œ**å¤§å¹…å‡å°‘æ— æ•ˆè®¡ç®—**ï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚

**ä»£ç ä½ç½®**ï¼š`verl/verl/workers/critic/dp_critic.py` > `update_critic`
```python
endpoint_mask = create_critic_endpoint_mask(response_mask, group_mask)
effective_mask = endpoint_mask.float() * response_mask.float()
vf_loss, vf_clipfrac = core_algos.compute_value_loss(
    vpreds=vpreds,
    values=values,
    returns=returns,
    response_mask=effective_mask,  # åªåœ¨ç«¯ç‚¹ä½ç½®è®¡ç®—
    ...
)
```

## 6. æ—¥å¿—ä¸ç›‘æ§
- **æ–°å¢**ï¼šè¯¦ç»†æ‰“å°æ¯ä¸ªresponseçš„ç«¯ç‚¹maskã€åˆ†ç»„ç»Ÿè®¡ï¼Œä¾¿äºè°ƒè¯•å’ŒéªŒè¯ç®—æ³•æ­£ç¡®æ€§ã€‚

**ä»£ç ä½ç½®**ï¼š`verl/verl/workers/critic/dp_critic.py` > `update_critic`
```python
print("endpoint_mask shape:", endpoint_mask.shape)
print("endpoint_mask sum per response:", endpoint_mask.sum(dim=1).tolist())
print("endpoint_mask total sum:", endpoint_mask.sum().item())
```

---

## æ€»ç»“
æœ¬æ¬¡GROUP_PPOçš„æ ¸å¿ƒæ”¹è¿›ç‚¹ï¼š
- **åˆ†ç»„maskå’Œç«¯ç‚¹maskç²¾ç»†åŒ–**ï¼Œå®ç°tokençº§åˆ†ç»„ä¸ç«¯ç‚¹æ•æ‰
- **advantageå¹¿æ’­ä¸åˆ†ç»„ä¸¥æ ¼å¯¹åº”**ï¼Œåˆ†ç»„å†…ä¸€è‡´ã€åˆ†ç»„é—´éš”ç¦»
- **Actoræ‰€æœ‰tokenå‚ä¸æ›´æ–°ï¼ŒCriticåªåœ¨ç«¯ç‚¹æ›´æ–°**ï¼Œè®­ç»ƒä¿¡å·å……åˆ†ä¸”é«˜æ•ˆ
- **ä¸Algorithm 2 Groupå®Œå…¨ä¸€è‡´ï¼Œç†è®ºä¸å·¥ç¨‹å®ç°åŒé‡å¯¹é½**

è¯¥å®ç°å·²å®Œå…¨è§£å†³åŸå§‹GROUP_PPOçš„åˆ†ç»„ã€ç«¯ç‚¹ã€å¹¿æ’­ç­‰å…³é”®é—®é¢˜ï¼Œæ¨èä½œä¸ºæ ‡å‡†å®ç°ã€‚

---

**æ¶‰åŠæ–‡ä»¶ä¸€è§ˆï¼š**
- `verl/verl/trainer/ppo/core_algos.py`
- `verl/verl/workers/actor/dp_actor.py`
- `verl/verl/workers/critic/dp_critic.py` 




